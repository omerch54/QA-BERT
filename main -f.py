# -*- coding: utf-8 -*-
"""Copy of F24 Project 2: Bert QA Stencil

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oMQfYq1w30GEAThCaisHPL7jv9UmPic6
"""

!pip install datasets==3.1.0
#load_dataset sometimes hangs on a higher version
!pip install transformers torch evaluate tqdm

"""# Preprocessing"""

from datasets import load_dataset

import torch
import numpy as np
import random

# we set up some seeds so that we can reproduce results
seed = 123
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

np.random.seed(seed)
random.seed(seed)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

"""
Some options for BERT model that can be run in colab:

"distilbert-base-uncased",
"distilbert-base-uncased-distilled-squad",
"distilbert-base-cased",
"distilbert-base-cased-distilled-squad",

"""
from google.colab import drive
drive.mount("/content/drive", force_remount=True)

# Change train.json / dev.json to the appropriate filepaths =====

data_files = {"train": "/content/drive/My Drive/Colab Notebooks/Copy of all_train.json", "dev": "/content/drive/My Drive/Colab Notebooks/Copy of all_dev.json"}
dataset = load_dataset('json', data_files=data_files)

from transformers import DistilBertTokenizerFast
from torch.utils.data import DataLoader, Dataset

# tokenization to process each example into a [CLS] Question [SEP] Context [SEP] format.

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')


class QADataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512, downsample_null=50):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Downsamplong null instances
        # if downsample_null > 1:
        #     null_examples = [example for example in self.data if not example['answers'][0].get('span_text')]
        #     non_null_examples = [example for example in self.data if example['answers'][0].get('span_text')]
        #     null_downsampled = random.sample(null_examples, len(non_null_examples) * downsample_null)
        #     self.data = non_null_examples + null_downsampled


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        example = self.data[idx]

        # Extract question and context
        question = example['questions'][0]['input_text']
        context = example['contexts']

        # Extract answer information
        answers = example.get('answers', [{}])[0]
        span_start = answers.get('span_start', 0)
        span_end = answers.get('span_end', 0)
        span_text = answers.get('span_text', "")

        # Tokenize the inputs
        inputs = self.tokenizer(
            question,
            context,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            # To map tokens back to character indices
            return_offsets_mapping=True,
            return_tensors="pt"
        )

        # Convert tokenized input to a dict and squeeze tensors
        inputs = {key: val.squeeze() for key, val in inputs.items()}
        # Extract offset mapping for span alignment
        offset_mapping = inputs.pop('offset_mapping')

        # Align character indices to token indices
        # Default to CLS token
        start_positions, end_positions = 0, 0
        if span_text and span_start is not None and span_end is not None:
            for idx, (start, end) in enumerate(offset_mapping):
                if start <= span_start < end:
                    start_positions = idx
                if start < span_end <= end:
                    end_positions = idx

        # Example Print Statements
        # Input: "This is the context."
        # Offset Mapping: [(0, 4), (5, 7), (8, 11), (12, 15), (16, 23), (23, 24)]
        # Each tuple (start, end) represents the character range of a token.

        # Example Input
        # Context: "This is the context."
        # Question: "What is this?"
        # Answer: "the" with span_start=8, span_end=11.

        # print(f"Offset Mapping: {offset_mapping}")
        # print(f"Span Start: {span_start}, Span End: {span_end}")
        # print(f"Mapped Start Position: {start_positions}, Mapped End Position: {end_positions}")

        # Offset Mapping: [(0, 4), (5, 7), (8, 11), (12, 15), (16, 23), (23, 24)]
        # Span Start: 8, Span End: 11
        # Mapped Start Position: 2, Mapped End Position: 2



        inputs['start_positions'] = torch.tensor(start_positions, dtype=torch.long)
        inputs['end_positions'] = torch.tensor(end_positions, dtype=torch.long)
        inputs['type_labels'] = torch.tensor(1 if span_text else 0, dtype=torch.long)
        # 1 = short answer, 0 = no answer

        return inputs

from transformers import DistilBertModel
import torch.nn as nn

# Fine-tuning a DistilBERT model with outputs for start, end, and type predictions

class BERTForQA(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')
        self.start_classifier = nn.Linear(self.bert.config.hidden_size, 1)
        self.end_classifier = nn.Linear(self.bert.config.hidden_size, 1)
        # 2 types: short answer or no answer
        self.type_classifier = nn.Linear(self.bert.config.hidden_size, 2)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state
        start_logits = self.start_classifier(hidden_states).squeeze(-1)
        end_logits = self.end_classifier(hidden_states).squeeze(-1)
        # [CLS] token
        type_logits = self.type_classifier(hidden_states[:, 0, :])
        return start_logits, end_logits, type_logits

def our_rank_spans(start_logits, end_logits, cls_logit):
    """
    Predict the best span or decide 'no answer' based on the logits.
    Args:
        start_logits: Tensor of shape (seq_len,) for start logits.
        end_logits: Tensor of shape (seq_len,) for end logits.
    Returns:
        (start_idx, end_idx): Predicted start and end token indices, or (0, 0) for no answer.
    """
    cls_score = start_logits[0] + end_logits[0]  # Score for 'no answer' (CLS token)

    best_span = (0, 0)  # Default to CLS
    best_score = cls_score

    for start_idx in range(len(start_logits)):
        for end_idx in range(start_idx, len(end_logits)):
            span_score = start_logits[start_idx] + end_logits[end_idx]
            if span_score > best_score:
                best_score = span_score
                best_span = (start_idx, end_idx)

    return best_span

# Computing the joint loss as stated in the paper
import torch.nn.functional as F

def compute_loss(start_logits, end_logits, type_logits, start_positions, end_positions, type_labels):
    # Calculate start loss
    start_log_probs = F.log_softmax(start_logits, dim=-1)  # Log probabilities
    start_loss = -start_log_probs[torch.arange(start_logits.size(0)), start_positions].mean()

    # Calculate end loss
    end_log_probs = F.log_softmax(end_logits, dim=-1)
    end_loss = -end_log_probs[torch.arange(end_logits.size(0)), end_positions].mean()

    # Calculate type loss
    type_log_probs = F.log_softmax(type_logits, dim=-1)
    type_loss = -type_log_probs[torch.arange(type_logits.size(0)), type_labels].mean()

    # Combine the losses
    total_loss = start_loss + end_loss + type_loss

    return total_loss

def compute_metrics(predictions, references):
    """
    Computes precision, recall, and F1 for span-level predictions.
    """
    true_positives = sum(1 for p, r in zip(predictions, references) if p == r and r != "no-answer")
    false_positives = sum(1 for p, r in zip(predictions, references) if p != r and r == "no-answer")
    false_negatives = sum(1 for p, r in zip(predictions, references) if p != r and p == "no-answer")

    precision = true_positives / (true_positives + false_positives + 1e-8)
    recall = true_positives / (true_positives + false_negatives + 1e-8)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)

    return precision, recall, f1

train_data = dataset['train']
dev_data = dataset['dev']
print(train_data[0])

# Create dataset instances
train_dataset = QADataset(train_data, tokenizer)
dev_dataset = QADataset(dev_data, tokenizer)

# Create DataLoaders
batch_size = 64
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)

from transformers import AdamW
from evaluate import load as load_metric
from tqdm.auto import tqdm


def train_loop(train_dataloader, dev_dataloader, model, optimizer, device):
    """
    Training loop for one epoch.
    Args:
        dataloader: DataLoader for the training dataset.
        model: BERTForQA model to train.
        optimizer: Optimizer for gradient updates.
        device: Device to run the model on ('cuda' or 'cpu').
    Returns:
        avg_loss: Average training loss over the epoch.
    """
    model.train()
    total_loss = 0
    # metrics = [load_metric(x) for x in ["accuracy"]]
    train_progress_bar = tqdm(train_dataloader, desc="Training")

    for i, batch in enumerate(train_progress_bar):
        # print(i)

        inputs = {key: val.to(device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)
        type_labels = batch['type_labels'].to(device)



        start_logits, end_logits, type_logits = model(**inputs)

        loss = compute_loss(start_logits, end_logits, type_logits, start_positions, end_positions, type_labels)

        optimizer.zero_grad()
        # Backpropagation
        loss.backward()
        optimizer.step()
        # lr_scheduler.step()

        total_loss += loss.item()
        # progress_bar.update(1)
        # if i==1:
        #   break

    avg_train_loss = total_loss / len(train_dataloader)

    model.eval()
    total_val_loss = 0
    val_progress_bar = tqdm(dev_dataloader, desc="Validating")

    with torch.no_grad():
        for batch in val_progress_bar:
            inputs = {key: val.to(device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)
            type_labels = batch['type_labels'].to(device)

            start_logits, end_logits, type_logits = model(**inputs)

            loss = compute_loss(start_logits, end_logits, type_logits, start_positions, end_positions, type_labels)

            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(dev_dataloader)

    return avg_train_loss, avg_val_loss

def eval_loop(dataloader, model, tokenizer, device):
    """
    Evaluate the model on the validation dataset and compute precision, recall, and F1 scores.
    Args:
        dataloader: DataLoader for validation data.
        model: Trained QA model.
        tokenizer: Tokenizer for decoding predictions (optional).
    Returns:
        precision, recall, f1: Span-level precision, recall, and F1 score.
    """
    model.eval()
    all_predictions = []
    all_references = []
    val_progress_bar = tqdm(dev_dataloader, desc="Validating")

    with torch.no_grad():
        for i, batch in enumerate(val_progress_bar):
            inputs = {key: val.to(device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)


            start_logits, end_logits, type_logits = model(**inputs)
            # print(i)

            # if i == 2:
            #   break


            for i in range(len(start_logits)):
                cls_logit = type_logits[i, 0].item()
                start_scores = start_logits[i]
                end_scores = end_logits[i]

                predicted_span = our_rank_spans(start_scores, end_scores, cls_logit)
                all_predictions.append(predicted_span)
                # print(all_predictions)

                true_span = (start_positions[i].item(), end_positions[i].item())
                all_references.append(true_span)
                # print(all_references)


    precision, recall, f1 = compute_metrics(all_predictions, all_references)
    return precision, recall, f1

def main():
  '''Here's the basic structure of the main block -- feel free to add or
  remove parameters/helper functions as you see fit, but all steps here are
  needed and we expect to see precision, recall, and f1 scores printed out'''
  device = "cuda" if torch.cuda.is_available() else "cpu"
  batch_size = 64
  num_epochs = 2
  learning_rate = 3e-5

  train_data = dataset['train']
  validation_data = dataset['dev']
  train_dataset = QADataset(train_data, tokenizer, downsample_null=50)
  validation_dataset = QADataset(validation_data, tokenizer)


  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
  validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)
  # print(f"Validation Dataset Size: {len(validation_dataset)}")

  model = BERTForQA().to(device)
  optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)

  # Training loop
  for epoch in range(num_epochs):
        # print(f"Epoch {epoch + 1}/{num_epochs}")

        avg_train_loss, avg_val_loss = train_loop(train_dataloader, validation_dataloader, model, optimizer, device)
        print(f"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

        # precision, recall, f1 = eval_loop(validation_dataloader, model, tokenizer, device)
        # print(f"Epoch {epoch + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")

  # Evaluation (assuming eval_loop is defined)
  precision, recall, f1 = eval_loop(validation_dataloader, model, tokenizer, device)
  print("PRECISION: ", precision)
  print("RECALL: ", recall)
  print("F1-SCORE: ", f1)

if __name__ == "__main__":
  main()